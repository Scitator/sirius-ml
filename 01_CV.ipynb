{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "01_CV.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BiLZ1Xguqmvm",
        "D8zcseRuqmvn",
        "uWHqxcASqmvp",
        "VAys9PIWqmvr",
        "wOYVyKOVqmvs",
        "gUvygdbJqmvt",
        "2lr9EQIYqmvu",
        "o88XdCWbqmvw"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp3aJdc0qmvf"
      },
      "source": [
        "# CV intro"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suKBg35Rqmvh"
      },
      "source": [
        "! pip install torch==1.8.0 \"catalyst[cv]\"==21.03 \"catalyst[ml]\"==21.03"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ3LnmDDqmvi"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wuq4oJWeqmvj"
      },
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAyX5U9Qqmvj"
      },
      "source": [
        "## Numpy and Pytorch intro\n",
        "### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtRQ5YVEqmvj"
      },
      "source": [
        "a = [1. , 1.4 , 2.5]\n",
        "print(f\"Simple way: {torch.tensor(a)}\")\n",
        "print(f\"Zeros:\\n {torch.zeros((2,3))}\")\n",
        "print(f\"Range: {torch.arange(0, 10)}\")\n",
        "print(f\"Complicated range: {torch.arange(4, 12, 2)}\")\n",
        "print(f\"Space: {torch.linspace(1, 4, 6)}\")\n",
        "print(f\"Identity matrix:\\n {torch.eye(4)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBThhGqGqmvk"
      },
      "source": [
        "### Random"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsKIoIpKqmvk"
      },
      "source": [
        "print(f\"From 0 to 1: {torch.rand(1)}\")\n",
        "print(f\"Vector from 0 to 1: {torch.rand(5)}\")\n",
        "print(f\"Vector from 0 to 10: {torch.randint(10, size=(5,))}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdAdWaUYqmvk"
      },
      "source": [
        "### Matrix Operation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3wFoDwYqmvl"
      },
      "source": [
        "a = torch.arange(10).type(torch.FloatTensor)\n",
        "b = torch.linspace(-10, 10, 10)\n",
        "print(f\"a: {a}\\nshape: {a.size()}\")\n",
        "print(f\"b: {a}\\nshape: {b.size()}\")\n",
        "print(f\"a + b: {a + b},\\n a * b: {a * b}\")\n",
        "print(f\"Dot product: {a.dot(b)}\")\n",
        "print(f\"Mean: {a.mean()}, STD: {a.std()}\")\n",
        "print(f\"Sum: {a.sum()}, Min: {a.min()}, Max: {a.max()}\")\n",
        "print(f\"Reshape:\\n{a.reshape(-1, 1)}\\nshape: {a.reshape(-1, 1).size()}\")\n",
        "c = a.reshape(-1, 1).repeat(1, 5)\n",
        "print(f\"Repeat:\\n{c}\\nshape: {c.size()}\")\n",
        "print(f\"Transpose:\\n{c.T}\\nshape: {c.T.size()}\")\n",
        "print(f\"Unique items: {torch.unique(c)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZvRFDToqmvl"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpH1f_2gqmvl"
      },
      "source": [
        "a = torch.arange(100).reshape(10, 10)\n",
        "print(f\"Array:\\n{a}\\nshape: {a.size()}\")\n",
        "print(f\"Get first column: {a[:, 0]}\")\n",
        "print(f\"Get last row: {a[-1, :]}\")\n",
        "print(f\"Add new awis:\\n{a[:, np.newaxis]}\\nshape: {a[:, np.newaxis].size()}\")\n",
        "print(f\"Specific indexing:\\n{a[4:6, 7:]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eStYQ5Gwqmvl"
      },
      "source": [
        "### Numpy <-> Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ2UxOQRqmvm"
      },
      "source": [
        "a = torch.normal(mean=torch.zeros(2,4))\n",
        "a.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUve_DFCqmvm"
      },
      "source": [
        "b = np.random.normal(size=(2, 4))\n",
        "torch.from_numpy(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiLZ1Xguqmvm"
      },
      "source": [
        "### CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHvGdHMUqmvm"
      },
      "source": [
        "a = torch.normal(mean=torch.zeros(2,4))\n",
        "b = torch.normal(mean=torch.zeros(2, 4))\n",
        "print(f\"a:\\n{a}\\nb:\\n{b}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMGa61yZqmvn"
      },
      "source": [
        "a = a.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG-puy27qmvn"
      },
      "source": [
        "a + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yYZvnTGqmvn"
      },
      "source": [
        "(a + b.cuda()).cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8zcseRuqmvn"
      },
      "source": [
        "### Autograd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFMuke73qmvn"
      },
      "source": [
        "a = torch.randn(2, requires_grad=True)\n",
        "b = torch.normal(mean=torch.zeros(2))\n",
        "\n",
        "c = torch.dot(a, b)\n",
        "print(f'a:\\n{a}\\nb:\\n{b}\\n(a,b): {c}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s-Ju3s-qmvo"
      },
      "source": [
        "c.backward()\n",
        "print(f'a:\\n{a}\\nb:\\n{b}\\n(a,b): {c}')\n",
        "print(f\"Grad a: {a.grad}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24u3Ot1iqmvo"
      },
      "source": [
        "a = torch.randn(2, requires_grad=True)\n",
        "b = torch.normal(mean=torch.zeros(2))\n",
        "c = torch.ones(1, requires_grad=True)\n",
        "\n",
        "d = torch.sigmoid(torch.dot(a, b) + c)\n",
        "print(f'a:\\n{a}\\nb:\\n{b}\\nSigmoid( (a,b) ): {d}')\n",
        "\n",
        "print(f\"Grad a: {a.grad}\\nGrad c: {c.grad}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuVqNKBxqmvo"
      },
      "source": [
        "d.backward()\n",
        "print(f\"Grad a: {a.grad}\\nGrad c: {c.grad}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNUypmjoqmvo"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swo28XSlqmvo"
      },
      "source": [
        "# PyTorch 101"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKS_tnE0qmvo"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from catalyst import dl\n",
        "from catalyst.data.transforms import ToTensor\n",
        "from catalyst.contrib.datasets import MNIST"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWHqxcASqmvp"
      },
      "source": [
        "### MNIST 101"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bmLZDpBqmvp"
      },
      "source": [
        "train_loader = DataLoader(\n",
        "    MNIST(os.getcwd(), train=True, download=True, transform=ToTensor()),\n",
        "    batch_size=32,\n",
        ")\n",
        "valid_loader = DataLoader(\n",
        "    MNIST(os.getcwd(), train=False, download=True, transform=ToTensor()),\n",
        "    batch_size=32,\n",
        ")\n",
        "\n",
        "model = nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 10)).cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i9gxOZ6qmvp"
      },
      "source": [
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "def train(model, optimizer, n_epoch=1, device=\"cpu\"):\n",
        "    train_logs = {\"Train Loss\": [0,], \"Steps\": [0,]}\n",
        "    valid_logs = {\"Valid Loss\": [0,], \"Valid Accuracy\": [0,], \"Steps\": [0,]}\n",
        "    step = 0\n",
        "    best_valid_loss = np.inf\n",
        "    best_model = None\n",
        "\n",
        "    for i in range(n_epoch):\n",
        "        # train\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            predictions = model(x_batch)\n",
        "            loss = criterion(predictions, y_batch)\n",
        "\n",
        "            loss.backward()\n",
        "            \n",
        "            optimizer.step()   \n",
        "            \n",
        "            step += 1\n",
        "            train_logs[\"Train Loss\"].append(loss.detach().item())\n",
        "            train_logs[\"Steps\"].append(step)\n",
        "\n",
        "        # valid\n",
        "        sum_loss = 0\n",
        "        sum_acc = 0\n",
        "        count_valid_steps = 0\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in valid_loader:\n",
        "                x_batch = x_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "\n",
        "                predictions = model(x_batch)\n",
        "                loss = criterion(predictions, y_batch)\n",
        "                sum_loss += loss.item()\n",
        "                sum_acc += accuracy_score(y_batch.cpu().numpy(), np.argmax(predictions.cpu().numpy(), axis=1))\n",
        "                count_valid_steps += 1\n",
        "\n",
        "            valid_logs[\"Valid Loss\"].append(sum_loss / count_valid_steps)\n",
        "            valid_logs[\"Valid Accuracy\"].append(sum_acc / count_valid_steps)\n",
        "            valid_logs[\"Steps\"].append(step)\n",
        "\n",
        "            if best_valid_loss > sum_loss / count_valid_steps:\n",
        "                best_valid_loss = sum_loss / count_valid_steps\n",
        "                best_model = deepcopy(model)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
        "    sns.lineplot(x=\"Steps\", y=\"Train Loss\", data=train_logs, ax=ax[0])\n",
        "    sns.lineplot(x=\"Steps\", y=\"Valid Loss\", data=valid_logs, ax=ax[1])\n",
        "    sns.lineplot(x=\"Steps\", y=\"Valid Accuracy\", data=valid_logs, ax=ax[2])\n",
        "    plt.plot()\n",
        "\n",
        "    return best_model, train_logs, valid_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycGAs9AUqmvq"
      },
      "source": [
        "model, _, _ = train(model, optimizer, n_epoch=2, device=\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKZXVfxMqmvq"
      },
      "source": [
        "# PyTorch is great, but...\n",
        "# sometimes it's too lowlevel\n",
        "# https://github.com/NVlabs/stylegan2-ada-pytorch/blob/main/training/training_loop.py#L88\n",
        "# https://github.com/NVlabs/imaginaire/blob/master/imaginaire/trainers/base.py#L24"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u74pxNtqmvq"
      },
      "source": [
        "# [Catalyst](https://github.com/catalyst-team/catalyst) 101\n",
        "PyTorch framework for Deep Learning research and development. It focuses on reproducibility, rapid experimentation, and codebase reuse so you can create something new rather than write another regular train loop.\n",
        "\n",
        "Break the cycle - use the Catalyst!\n",
        "\n",
        "tl;dr\n",
        "- [minimal examples](https://github.com/catalyst-team/catalyst#minimal-examples)\n",
        "- [docs](https://catalyst-team.github.io/catalyst/)\n",
        "- [Config API](https://github.com/catalyst-team/catalyst/tree/master/examples/mnist_stages) (advanced)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZU1hu9iqmvr"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from catalyst import dl\n",
        "from catalyst.data.transforms import ToTensor\n",
        "from catalyst.contrib.datasets import MNIST"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73xzdVs2qmvr"
      },
      "source": [
        "train_loader = DataLoader(\n",
        "    MNIST(os.getcwd(), train=True, download=True, transform=ToTensor()),\n",
        "    batch_size=32,\n",
        ")\n",
        "valid_loader = DataLoader(\n",
        "    MNIST(os.getcwd(), train=False, download=True, transform=ToTensor()),\n",
        "    batch_size=32,\n",
        ")\n",
        "loaders = {\"train\": train_loader, \"valid\": valid_loader}\n",
        "\n",
        "model = nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 10))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAys9PIWqmvr"
      },
      "source": [
        "### MNIST 101"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccz8MmQ-qmvr",
        "scrolled": true
      },
      "source": [
        "runner = dl.SupervisedRunner(input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\")\n",
        "# model training\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    loaders=loaders,\n",
        "    num_epochs=5,\n",
        "    callbacks=[\n",
        "        dl.AccuracyCallback(input_key=\"logits\", target_key=\"targets\", topk_args=(1, 3, 5)),\n",
        "    ],\n",
        "    logdir=\"./logs/mnist_101\",\n",
        "    valid_loader=\"valid\",\n",
        "    valid_metric=\"loss\",\n",
        "    minimize_valid_metric=True,\n",
        "    verbose=True,\n",
        "    load_best_on_end=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOYVyKOVqmvs"
      },
      "source": [
        "### MNIST 102"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUslcvFVqmvs",
        "scrolled": true
      },
      "source": [
        "runner = dl.SupervisedRunner(input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\")\n",
        "# model training\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    loaders=loaders,\n",
        "    num_epochs=5,\n",
        "    logdir=\"./logs/mnist_102\",\n",
        "    callbacks=[\n",
        "        dl.AccuracyCallback(input_key=\"logits\", target_key=\"targets\", num_classes=10, log_on_batch=True),\n",
        "        dl.PrecisionRecallF1SupportCallback(input_key=\"logits\", target_key=\"targets\", num_classes=10, log_on_batch=True),\n",
        "        dl.AUCCallback(input_key=\"logits\", target_key=\"targets\"),\n",
        "        # catalyst[ml] required\n",
        "        dl.ConfusionMatrixCallback(input_key=\"logits\", target_key=\"targets\", num_classes=10),\n",
        "        dl.CheckpointCallback(\n",
        "            logdir=\"./logs/mnist_102\", \n",
        "            loader_key=\"valid\", \n",
        "            metric_key=\"accuracy01\", \n",
        "            minimize=False, \n",
        "            save_n_best=3, \n",
        "            load_on_stage_end=\"best\"\n",
        "        ),\n",
        "        dl.TqdmCallback()\n",
        "    ],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFL-T6GPqmvs"
      },
      "source": [
        "# complicated console output?\n",
        "# time for an advanced logging!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAn4YWU3qmvs"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgu-U3ovqmvs"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha6HKUsyqmvt"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfoRblkaqmvt"
      },
      "source": [
        "# Catalyst 102"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUvygdbJqmvt"
      },
      "source": [
        "### MNIST metric learning\n",
        "- [Metric Learning with Catalyst](https://medium.com/pytorch/metric-learning-with-catalyst-8c8337dfab1a)\n",
        "- [Representation Learning with Catalyst and Faces](https://medium.com/catalyst-team/representation-learning-with-catalyst-and-faces-946644d49184)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmFqkyL7qmvt"
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "import os\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from catalyst import data, dl\n",
        "from catalyst.contrib import datasets, models, nn\n",
        "from catalyst.data.transforms import Compose, Normalize, ToTensor\n",
        "\n",
        "\n",
        "# 1. train and valid loaders\n",
        "transforms = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "train_dataset = datasets.MnistMLDataset(root=os.getcwd(), download=True, transform=transforms)\n",
        "sampler = data.BalanceBatchSampler(labels=train_dataset.get_labels(), p=5, k=10)\n",
        "train_loader = DataLoader(dataset=train_dataset, sampler=sampler, batch_size=sampler.batch_size)\n",
        "\n",
        "valid_dataset = datasets.MnistQGDataset(root=os.getcwd(), transform=transforms, gallery_fraq=0.2)\n",
        "valid_loader = DataLoader(dataset=valid_dataset, batch_size=1024)\n",
        "\n",
        "# 2. model and optimizer\n",
        "model = models.MnistSimpleNet(out_features=16)\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 3. criterion with triplets sampling\n",
        "sampler_inbatch = data.HardTripletsSampler(norm_required=False)\n",
        "criterion = nn.TripletMarginLossWithSampler(margin=0.5, sampler_inbatch=sampler_inbatch)\n",
        "\n",
        "# 4. training with catalyst Runner\n",
        "class CustomRunner(dl.SupervisedRunner):\n",
        "    def handle_batch(self, batch) -> None:\n",
        "        if self.is_train_loader:\n",
        "            images, targets = batch[\"features\"].float(), batch[\"targets\"].long()\n",
        "            features = self.model(images)\n",
        "            self.batch = {\"embeddings\": features, \"targets\": targets,}\n",
        "        else:\n",
        "            images, targets, is_query = batch[\"features\"].float(), batch[\"targets\"].long(), batch[\"is_query\"].bool()\n",
        "            features = self.model(images)\n",
        "            self.batch = {\"embeddings\": features, \"targets\": targets, \"is_query\": is_query}\n",
        "\n",
        "callbacks = [\n",
        "    dl.ControlFlowCallback(\n",
        "        dl.CriterionCallback(\n",
        "            input_key=\"embeddings\", target_key=\"targets\", metric_key=\"loss\"\n",
        "        ),\n",
        "        loaders=\"train\",\n",
        "    ),\n",
        "    dl.ControlFlowCallback(\n",
        "        dl.CMCScoreCallback(\n",
        "            embeddings_key=\"embeddings\",\n",
        "            labels_key=\"targets\",\n",
        "            is_query_key=\"is_query\",\n",
        "            topk_args=[1],\n",
        "        ),\n",
        "        loaders=\"valid\",\n",
        "    ),\n",
        "    dl.PeriodicLoaderCallback(\n",
        "        valid_loader_key=\"valid\", valid_metric_key=\"cmc01\", minimize=False, valid=2\n",
        "    ),\n",
        "    dl.CheckpointCallback(logdir=\"./logs/ml\", loader_key=\"valid\", metric_key=\"cmc01\", minimize=False)\n",
        "]\n",
        "\n",
        "runner = CustomRunner(input_key=\"features\", output_key=\"embeddings\")\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    callbacks=callbacks,\n",
        "    loaders={\"train\": train_loader, \"valid\": valid_loader},\n",
        "    verbose=False,\n",
        "    logdir=\"./logs/ml\",\n",
        "#     valid_loader=\"valid\",\n",
        "#     valid_metric=\"cmc01\",\n",
        "#     minimize_valid_metric=False,\n",
        "    num_epochs=10,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lr9EQIYqmvu"
      },
      "source": [
        "### MNIST GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYZvXDWRqmvu"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from catalyst import dl\n",
        "from catalyst.contrib.datasets import MNIST\n",
        "from catalyst.contrib.nn.modules import Flatten, GlobalMaxPool2d, Lambda\n",
        "from catalyst.data.transforms import ToTensor\n",
        "\n",
        "latent_dim = 128\n",
        "generator = nn.Sequential(\n",
        "    # We want to generate 128 coefficients to reshape into a 7x7x128 map\n",
        "    nn.Linear(128, 128 * 7 * 7),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    Lambda(lambda x: x.view(x.size(0), 128, 7, 7)),\n",
        "    nn.ConvTranspose2d(128, 128, (4, 4), stride=(2, 2), padding=1),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    nn.ConvTranspose2d(128, 128, (4, 4), stride=(2, 2), padding=1),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    nn.Conv2d(128, 1, (7, 7), padding=3),\n",
        "    nn.Sigmoid(),\n",
        ")\n",
        "discriminator = nn.Sequential(\n",
        "    nn.Conv2d(1, 64, (3, 3), stride=(2, 2), padding=1),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    nn.Conv2d(64, 128, (3, 3), stride=(2, 2), padding=1),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    GlobalMaxPool2d(),\n",
        "    Flatten(),\n",
        "    nn.Linear(128, 1),\n",
        ")\n",
        "\n",
        "model = {\"generator\": generator, \"discriminator\": discriminator}\n",
        "criterion = {\"generator\": nn.BCEWithLogitsLoss(), \"discriminator\": nn.BCEWithLogitsLoss()}\n",
        "optimizer = {\n",
        "    \"generator\": torch.optim.Adam(generator.parameters(), lr=0.0003, betas=(0.5, 0.999)),\n",
        "    \"discriminator\": torch.optim.Adam(discriminator.parameters(), lr=0.0003, betas=(0.5, 0.999)),\n",
        "}\n",
        "loaders = {\"train\": DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=ToTensor()), batch_size=32)}\n",
        "\n",
        "class CustomRunner(dl.Runner):\n",
        "    def predict_batch(self, batch):\n",
        "        batch_size = 1\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = torch.randn(batch_size, latent_dim).to(self.device)\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.model[\"generator\"](random_latent_vectors).detach()\n",
        "        return generated_images\n",
        "\n",
        "    def handle_batch(self, batch):\n",
        "        real_images, _ = batch\n",
        "        batch_size = real_images.shape[0]\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = torch.randn(batch_size, latent_dim).to(self.device)\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.model[\"generator\"](random_latent_vectors).detach()\n",
        "        # Combine them with real images\n",
        "        combined_images = torch.cat([generated_images, real_images])\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = torch.cat([torch.ones((batch_size, 1)), torch.zeros((batch_size, 1))]).to(self.device)\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * torch.rand(labels.shape).to(self.device)\n",
        "\n",
        "        # Discriminator forward\n",
        "        combined_predictions = self.model[\"discriminator\"](combined_images)\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = torch.randn(batch_size, latent_dim).to(self.device)\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = torch.zeros((batch_size, 1)).to(self.device)\n",
        "\n",
        "        # Generator forward\n",
        "        generated_images = self.model[\"generator\"](random_latent_vectors)\n",
        "        generated_predictions = self.model[\"discriminator\"](generated_images)\n",
        "\n",
        "        self.batch = {\n",
        "            \"combined_predictions\": combined_predictions,\n",
        "            \"labels\": labels,\n",
        "            \"generated_predictions\": generated_predictions,\n",
        "            \"misleading_labels\": misleading_labels,\n",
        "        }\n",
        "\n",
        "\n",
        "runner = CustomRunner()\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    loaders=loaders,\n",
        "    callbacks=[\n",
        "        dl.CriterionCallback(\n",
        "            input_key=\"combined_predictions\",\n",
        "            target_key=\"labels\",\n",
        "            metric_key=\"loss_discriminator\",\n",
        "            criterion_key=\"discriminator\",\n",
        "        ),\n",
        "        dl.CriterionCallback(\n",
        "            input_key=\"generated_predictions\",\n",
        "            target_key=\"misleading_labels\",\n",
        "            metric_key=\"loss_generator\",\n",
        "            criterion_key=\"generator\",\n",
        "        ),\n",
        "        dl.OptimizerCallback(model_key=\"generator\", optimizer_key=\"generator\", metric_key=\"loss_generator\"),\n",
        "        dl.OptimizerCallback(model_key=\"discriminator\", optimizer_key=\"discriminator\", metric_key=\"loss_discriminator\"),\n",
        "    ],\n",
        "    valid_loader=\"train\",\n",
        "    valid_metric=\"loss_generator\",\n",
        "    minimize_valid_metric=True,\n",
        "    num_epochs=20,\n",
        "    verbose=True,\n",
        "    logdir=\"./logs/gan\",\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty-HCyZVqmvw"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(runner.predict_batch(None)[0, 0].cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o88XdCWbqmvw"
      },
      "source": [
        "### Mnist funetuting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX_6bOEJqmvw"
      },
      "source": [
        "[![Catalyst logo](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/third_party_pics/train-loop.png)](https://github.com/catalyst-team/catalyst#minimal-examples)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WyffKRmqmvx"
      },
      "source": [
        "import os\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from catalyst import dl, utils\n",
        "from catalyst.contrib.datasets import MNIST\n",
        "from catalyst.data.transforms import ToTensor\n",
        "\n",
        "\n",
        "class CustomRunner(dl.IRunner):\n",
        "    def __init__(self, logdir, device):\n",
        "        super().__init__()\n",
        "        self._logdir = logdir\n",
        "        self._device = device\n",
        "\n",
        "    def get_engine(self):\n",
        "        return dl.DeviceEngine(self._device)\n",
        "\n",
        "    def get_loggers(self):\n",
        "        return {\n",
        "            \"console\": dl.ConsoleLogger(),\n",
        "            \"csv\": dl.CSVLogger(logdir=self._logdir),\n",
        "            \"tensorboard\": dl.TensorboardLogger(logdir=self._logdir),\n",
        "        }\n",
        "\n",
        "    @property\n",
        "    def stages(self):\n",
        "        return [\"train_freezed\", \"train_unfreezed\"]\n",
        "\n",
        "    def get_stage_len(self, stage: str) -> int:\n",
        "        return 3\n",
        "\n",
        "    def get_loaders(self, stage: str, epoch: int = None):\n",
        "        loaders = {\n",
        "            \"train\": DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=ToTensor()), batch_size=32),\n",
        "            \"valid\": DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=ToTensor()), batch_size=32),\n",
        "        }\n",
        "        return loaders\n",
        "\n",
        "    def get_model(self, stage: str):\n",
        "        model = (\n",
        "            self.model\n",
        "            if self.model is not None\n",
        "            else nn.Sequential(nn.Flatten(), nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10))\n",
        "        )\n",
        "        if stage == \"train_freezed\":\n",
        "            # freeze layer\n",
        "            utils.set_requires_grad(model[1], False)\n",
        "        else:\n",
        "            utils.set_requires_grad(model, True)\n",
        "        return model\n",
        "\n",
        "    def get_criterion(self, stage: str):\n",
        "        return nn.CrossEntropyLoss()\n",
        "\n",
        "    def get_optimizer(self, stage: str, model):\n",
        "        if stage == \"train_freezed\":\n",
        "            return optim.Adam(model.parameters(), lr=1e-3)\n",
        "        else:\n",
        "            return optim.SGD(model.parameters(), lr=1e-1)\n",
        "\n",
        "    def get_scheduler(self, stage: str, optimizer):\n",
        "        return None\n",
        "\n",
        "    def get_callbacks(self, stage: str):\n",
        "        return {\n",
        "            \"criterion\": dl.CriterionCallback(\n",
        "                metric_key=\"loss\", input_key=\"logits\", target_key=\"targets\"\n",
        "            ),\n",
        "            \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"),\n",
        "            # \"scheduler\": dl.SchedulerCallback(loader_key=\"valid\", metric_key=\"loss\"),\n",
        "            # \"accuracy\": dl.AccuracyCallback(input_key=\"logits\", target_key=\"targets\", topk_args=(1, 3, 5)),\n",
        "            # \"classification\": dl.PrecisionRecallF1SupportCallback(input_key=\"logits\", target_key=\"targets\", num_classes=10),\n",
        "            # \"confusion_matrix\": dl.ConfusionMatrixCallback(input_key=\"logits\", target_key=\"targets\", num_classes=10),\n",
        "            \"checkpoint\": dl.CheckpointCallback(self._logdir, loader_key=\"valid\", metric_key=\"loss\", minimize=True, save_n_best=3),\n",
        "        }\n",
        "\n",
        "    def handle_batch(self, batch):\n",
        "        x, y = batch\n",
        "        logits = self.model(x)\n",
        "\n",
        "        self.batch = {\n",
        "            \"features\": x,\n",
        "            \"targets\": y,\n",
        "            \"logits\": logits,\n",
        "        }\n",
        "\n",
        "runner = CustomRunner(\"./logs/finetuning\", \"cuda\")\n",
        "runner.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOojsH92qmvx"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahVydkhIw_ti"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-ei8FTFxCLz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}